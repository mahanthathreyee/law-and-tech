```table-of-contents
```
# Mozilla v FCC

## Petitioner's Brief

> Are the Petitioners for or against net neutrality?

Petitioners are **FOR** net neutrality.

> What are the main arguments/support for the Petitioners position?

- The FCC wrongly reclassified BIAS from a "telecommunications service" to an "information service," against statutory definitions
- The FCC justified reclassifying BIAS by arguing that since BIAS allows users to access third-party information services, BIAS itself is an information service. 
	- This rationale fails to distinguish between the service's role in providing access to the internet (a telecommunications function) and the nature of the content and services accessed, which are the actual "information services."
- The FCC improperly used functions like DNS and caching to label BIAS as an information service, ignoring their ancillary nature.
- The FCC's repeal of open internet rules neglects its duty to protect public interest and maintain an open internet.
	- This decision undermines the foundational principle of net neutrality, which ensures that BIAS providers do not block, throttle, or unfairly prioritize internet traffic.

## FCC's Brief

> Is FCC for or against net neutrality (in relation to this reading)?

FCC is **AGAINST** net neutrality.

> What are the main arguments/support for the FCC's position?

- The FCC decided to treat broadband as a lighter-regulated "information service," believing this encourages more innovation and investment than the previous, stricter "telecommunications service" classification.
- The FCC ruled that mobile internet is not a "commercial mobile service" needing strict regulation, aiming for consistent treatment with fixed broadband to not over-regulate.
- With broadband reclassified, the FCC removed strict rules, opting for a simpler approach that relies on companies being transparent and existing laws to prevent abuse, which is seen as more efficient.
- The FCC believes that less strict regulation will lead to more broadband investment and doesn't see the need for old rules because competition and legal protections against abuse already exist.
- The FCC decided to override any state or local laws that are stricter than its own, to ensure the internet is governed uniformly across the U.S., preventing a mix of different rules in different places.

# Autonomous Weapons System

> How are autonomous weapons similar and different compared to:
> 1. conventional weapons
> 2. human combatants
> 3. child soldiers
> 4. animal combatants

| **Aspect**             | **Conventional Weapons**                                                                                                          | **Human Combatants**                                                                                                                                         | **Child Soldiers**                                                                                                                | **Animal Combatants**                                                                                                                                 |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Similarity**         | AWS are like conventional weapons in that they can be lawful under specific conditions (e.g., not causing unnecessary suffering). | AWS and human combatants share the capability for independent action on the battlefield.                                                                     | AWS, child soldiers, and animal combatants are capable of autonomous action and may act unpredictably.                            | Similar to child soldiers, AWS and animal combatants can act autonomously and unpredictably.                                                          |
| **Difference**         | AWS's capacity for independent and unpredictable action complicates their legal review and accountability.                        | Unlike humans, AWS cannot be traditionally trained or held accountable for their actions.                                                                    | The legal and societal motivations for regulating child soldiers (to protect children) do not apply to AWS.                       | The rationale for regulating animal combatants does not directly apply to AWS, and there's almost no existing law specifically for animal combatants. |
| **Accountability Gap** | N/A                                                                                                                               | AWS actions that result in violations of international humanitarian law highlight an accountability gap not addressed by the law governing human combatants. | Similar accountability issues as with human combatants; existing frameworks do not accommodate autonomous systems' liability.     | Similar to child soldiers, the unique challenges of AWS accountability are not resolved within the current legal frameworks.                          |
| **Regulation Needs**   | Existing weapons law does not fully address AWS's unique challenges, such as in-field learning and ensuring discriminate use.     | Current combatant law fails to provide guidelines on "training" AWS or addressing the accountability gap for their autonomous actions.                       | Child soldier regulations highlight the need for tailored AWS laws that consider their non-human nature and operational autonomy. | The lack of specific law for animal combatants underscores the necessity for new regulations specifically designed for AWS.                           |

# Trolley and Pinto Problem

> What was the Pinto Problem? 

This problem is exemplified by the controversy surrounding the Ford Pinto in the 1970s, where Ford Motor Company faced criticism for its decision-making process regarding the safety of the vehicle's fuel tank design. Despite being aware of the hazards related to the design, Ford adhered to the original design based on a cost-benefit analysis that compared the costs of making safety improvements against the benefits of preventing fuel tank fires. This analysis calculated the monetary value of human lives and injuries in a way that suggested a willingness to accept human casualties in favor of cost savings.

> How is the Pinto problem similar/different to the Boeing 737 MAX case?

- **Technical and Contextual Nature of Failures:**
    - The Pinto problem was related to a specific design flaw (fuel tank design) that made the car susceptible to catching fire in rear-end collisions.
    - The 737 MAX issue involved a complex flight control system (MCAS) designed to enhance the aircraft's performance, which malfunctioned due to sensor errors and lack of adequate pilot training and information.

- **Regulatory and Industry Environment:**
    - The automotive industry regulations and standards at the time of the Pinto were less comprehensive in addressing safety issues compared to the highly regulated aviation sector during the 737 MAX's development.
    - The Boeing 737 MAX case also involved criticisms of the Federal Aviation Administration (FAA) for its certification process and alleged closeness with Boeing, which was not a significant aspect of the Pinto case.

- **Complexity of Technology and Information Disclosure:**
    - The Boeing 737 MAX's problems were partially attributed to the complexity of modern aviation systems and Boeing's failure to disclose critical information about the MCAS system to pilots and airlines.
    - The Pinto's design issue, by contrast, was a more straightforward safety risk that did not involve the complexities of software or system-wide integration failures.

- **Global Impact and Scale:**
    - The Boeing 737 MAX incidents had a global impact, grounding the worldwide fleet and affecting international air travel, whereas the Pinto issue primarily affected consumers in the United States.
    - The scale of the 737 MAX crisis involved multiple international investigations and a broader scrutiny of global aviation safety practices, while the Pinto controversy was more contained within the automotive industry and national regulatory responses.

# Free Speech in an Algorithmic Society

> Why is Professor Balkin worried about freedom of speech in the current era?

Professor Balkin is concerned about freedom of speech due to the pervasive influence of Big Data and new forms of speech regulation. 

He sees Big Data as a power structure where personal information becomes a commodity, likened to "Soylent Green," leading to surveillance, control, discrimination, and manipulation by both governments and private companies. 

Additionally, the evolution from traditional to new forms of speech regulation, exemplified by the right to be forgotten and the spread of fake news, challenges traditional free speech protections, raising issues around censorship, privacy, and misinformation in the digital age.

> Who does Professor Balkin believe must play a role in preserving freedom of speech?

The best solution would be for large international infrastructure owners and social media platforms to change their self-conception. 

Ideally, they would come to understand themselves as a new kind of media company, with obligations to protect the global public good of a free Internet, and to preserve and extend the emerging global system of freedom of expression. 

Defenders of democratic values should work hard to emphasize the social responsibilities of digital infrastructure companies and help them both to understand and to accept their constitutive role in the emerging global public sphere.

> What are information fiduciaries? What is their role?

Information fiduciaries are entities that have a duty to protect the personal information of individuals they interact with, similar to the fiduciary responsibilities of doctors or lawyers to their clients. 

They must act in the best interests of those whose data they hold, ensuring confidentiality and preventing misuse. This concept suggests that companies collecting and handling significant amounts of personal data should be regulated to enforce ethical standards of trust, confidentiality, and care towards individuals' data.

> Why can't digital information fiduciaries should have fewer obligations than traditional professional fiduciaries according to the author?

1. Their business significantly involves monetizing personal data, unlike traditional fiduciaries.
2. Digital platforms encourage public expression and content creation, affecting how user data is handled and expected to be protected.
3. The level of trust and care expected from digital platforms by users is different; users do not expect digital platforms to protect their interests to the same extent as traditional fiduciaries like doctors or lawyers.

> What is trolling and doxing?

**Trolling:** Refers to the act of deliberately posting provocative or offensive content online to elicit a strong emotional response or disrupt conversations.

**Doxing:** Researching and publicly sharing private or identifying information about an individual without their consent, often to harass, intimidate, or shame them.

> How does Professor Balkin want potentially harmful online phenomena, like trolling and doxing, to be addressed, and by whom?

It must be addressed by the internet companies but both the government and users have a vital role to play in mitigating these harms.

- It is in the best interest of the internet companies to protect their users from harm.
    - If users are harmed, they may choose to leave the platform and its reputation might be harmed
	- These harms can also impact value of the company and could make their shareholders unhappy
- There must be legislative and administrative regulation and technological design to prevent and protect such harm
- Government and users will pressure these companies to prevent these harms
- These companies could place community wide policies to foster positive engagement and discouraging harmful behaviors.

> What is an "algorithmic nuisance" and how should this phenomenon be addressed?

**Algorithmic nuisance:** Refers to the negative impact of companies using big data and algorithms to make judgments that affect people's identities, traits, associations, opportunities, and vulnerabilities. It captures the idea of companies externalizing the socially unjustified costs of their decision-making onto individuals. 

To address this phenomenon, it's suggested that there should be regulations aimed at the outputs of algorithmic decision-making, such as discrimination and manipulation, ensuring companies internalize the costs they impose on society and individuals through their practices.

# Big Data's Disparate Impact

> What is disparate treatment?

Unequal treatment of similarly situated people based on a protected class (race, gender, etc.). Disparate treatments comprises of two forms:
- **Formal Discrimination:** Membership in a protected class is used as an input to the model. It covers both the straightforward denial of opportunities based on protected class membership and the use of rational racism.
- **Discriminatory Intent:** Requires evidence that the different treatment was **motivated** by the employee's membership in a protected class, such as race, gender, age, disability, religion, or national origin.

> What is disparate impact?

Unintentional discrimination that occurs when a facially neutral policy or practice disadvantages a protected class (e.g., race, gender, age) to a greater extent than another group.

> What is masking & problems of proof?

**Masking:**

- Masking refers to concealing the intention behind discriminatory decisions in algorithmic decision-making.
- It can involve using data mining to create models that unintentionally or intentionally discriminate, with the process obscuring the discrimination's source.
- Discrimination can be masked by using proxies for protected characteristics, relying on biased historical data, or selecting features that disadvantage specific groups.

**Problems of Proof:**

- Demonstrating discrimination, especially under legal frameworks like disparate impact or treatment, is challenging due to the opacity and complexity of algorithms.
- The indirect ways discrimination manifests in data mining exacerbate these challenges, making it difficult to identify and prove the source of bias or discriminatory intent.
- The article highlights the need for more transparency, accountability, and rigor in algorithmic decision-making to address and prove discrimination effectively.

> Defining the target variable

- Defines what data miners are looking for.
- The process of defining the target variable is crucial in data mining as it sets the goal or outcome that the algorithm seeks to predict or classify.

**Issues:**
- The definition of the target variable is inherently subjective and can introduce biases based on how the problem is framed and understood by those setting up the data mining process.
- Risk of inheriting past biases and discrimination through reliance on historical data.
- The chosen target variable may fail to fully capture the diversity and variations in real-world outcomes.
- Neglect of qualitative aspects in favor of easily measured indicators. leading to outcomes that favor easily measured success indicators over a more holistic understanding of the issue.

> Training data (labeling and data collection)?

**Training Data**
- The dataset used to train the algorithm to make predictions or classifications.
- Consists of examples that the model learns from to understand patterns or regularities.

**Labelling**
- The process of assigning class labels to examples in the training data.
- Crucial for supervised learning, as it guides the algorithm in learning the relationship between input features and the desired output.

**Data Collection**
- Involves gathering the data that will be used for training the model.
- Can include a wide range of sources and methods for acquiring data.

**Issues:**
- Training data may inherit biases from historical decision-making or societal prejudices, leading to biased algorithm outcomes.
- Errors in labeling can misguide the model, leading to inaccurate predictions or classifications.
- Training data that do not accurately represent the diversity of the population or scenario can result in models that perform poorly or unfairly for underrepresented groups.
- Poor data quality, including inaccuracies, incompleteness, or outdated information, can negatively impact model performance and fairness.
- The method of data collection can introduce bias, for example, through the overrepresentation or underrepresentation of certain groups.

> What is feature selection? What are issues related to it?

**Feature Selection**
- The process of identifying and selecting a subset of relevant features (variables, predictors) for use in model construction.
- Aims to improve model performance by including only the most informative, relevant features and excluding redundant or irrelevant data.

**Issues**
- Choosing features that inadvertently exclude or include data can introduce or perpetuate bias in model outcomes
- Features selected may serve as proxies for protected characteristics (e.g., zip code as a proxy for race), leading to indirect discrimination.
- Failure to include features that capture crucial nuances or variations can result in models that do not fully understand or accurately predict outcomes for all groups.
- Selecting features at too coarse a level of detail can mask important distinctions and nuances, impacting model fairness and accuracy.

 > What are proxies? What are issues related to it?
 
 **Proxies:** Proxies in data mining refer to attributes or features used in models that serve as stand-ins for other variables, including protected characteristics like race, gender, or age.

**Issues:**
- **Indirect Discrimination:** Using proxies can lead to indirect discrimination where models make decisions based on attributes closely related to protected characteristics.
- **Unintentional Bias:** Even without explicit intention to discriminate, reliance on proxies can introduce bias into decision-making processes, affecting fairness and equity.
- **Opacity in Decision-Making:** The use of proxies can obscure the true basis for decisions, making it difficult to identify, understand, and address discriminatory practices.
- **Challenge in Identifying Proxies:** Determining what constitutes a proxy can be complex, as many variables could potentially serve as indirect indicators of protected characteristics.

> Why doesn't Title VII apply for data mining cases?

- **Hard to Prove:** Title VII requires intentional discrimination proof, but data mining can perpetuate bias unintentionally.
	- **Biased Predictions:** Data mining finds correlations that might reflect historical bias, making them difficult to challenge as legitimate job predictors. 
- **Business Necessity:** Discriminatory outcomes can be justified if data mining shows they predict job performance. Proving data correlations reflect bias, not job fit, is difficult.
- **Complex Solutions:** Fixing bias involves technical adjustments and legal challenges in proving discrimination and proposing fair solutions.
- **Competing Goals:** Balancing antidiscrimination rules (protected characteristics) with addressing societal inequalities creates legal and political hurdles.

# Punishing Artificial Intelligence

> What are features of AI?

1. AI has the potential to act unpredictably.
2. AI has the potential to act unexplainably. It may be possible to determine what an AI has done, but not how or why it acted as it did.
3. AI may act autonomously. For our purposes, that is to say an AI may cause harm without being directly controlled by an individual.
4. While AI can already outperform people in spectacular fashion in some domains, like board games, in other domains AI is not competitive with toddlers. That is because all AI is designed to perform “narrow” or “specific” tasks.

> Why is reducibility important in AI crimes?

Reducibility is also critical, because if an AI engages in an act that would be criminal for a person and the act is reducible, then there typically will be a person that could be criminally liable.

If an AI act is not effectively reducible, there may be no other party that is aptly punished, in which case intuitively criminal activity could occur without the possibility of punishment.

> What are few scenarios where AI crimes are not reducible?

- **Enforcement Problems:** Identifying the individual responsible for an AI-related crime can be difficult if the perpetrator remains anonymous, such as the creator of a computer virus.
    
- **Practical Irreducibility:** Legal attempts to attribute AI misconduct to specific individuals may be impractical due to the multitude of contributors, the challenge in pinpointing their roles, or their actions being remote in time or location.
    
- **Legal Irreducibility:** Legally, attributing AI crimes to individual actions might not always be advisable. Minor negligent acts by multiple people could cumulatively cause significant harm through AI, but prosecuting these actions might not align with sound criminal policy.

> What is specific and general deterrence?

Specific deterrence is the process whereby punishing a specific individual discourages that person from committing more crime in the future.

General deterrence occurs when punishing an offender discourages other would-be offenders from committing crimes.

> What is the desert constraint?

The desert constraint is the claim that an offender may not, in justice, be punished in excess of his or her desert. 

The main effect of the desert constraint is to rule out punishments that go beyond what is proportionate to one’s culpability. Thus, it would be wrong to execute someone for jaywalking even if doing so would ultimately save lives by reducing illegal and dangerous pedestrian crossings.

> What is an example of an affirmative case for punishing AI?

- Direct punishment of AI could provide unrestricted general deterrence as against the developers, owners, or users of AI and provide incentives for them to avoid creating AIs that cause especially egregious types of harm without excuse or justification.
- Punishment of AI may also have expressive benefits. Expressing condemnation of the harms suffered by the victims of an AI could provide these victims with a sense of satisfaction and vindication.

> What is an issue of punishing AI as a means of expressing condemnation?

Punishing AI could send the message that AI is itself an actor on par with a human being, which is responsible and can be held accountable through the criminal justice system. Such a message is concerning, as it could entrench the view that AI has rights to certain kinds of benefits, protections and dignities that could restrict valuable human activities.

> What is a retributive limitation of this case?

There are 3 limitations:

**Eligibility challenge**

The Eligibility Challenge is simple to state: AI, like other inanimate objects, is not the right kind of thing to be punished. AI lacks mental states and thus cannot fulfill the mental state (mens rea) elements built into most criminal offenses. Therefore, convicting AI of crimes requiring a mens rea like intent, knowledge or recklessness would violate the principle of legality.

There are 3 ways to address this challenge:
1. **Respondent Superior:** Suggests that culpable mental states of human actors (developers, owners) associated with an AI could be imputed to the AI under certain conditions, making the AI liable for crimes.
	- Doesn't work for hard AI crimes where actors behind the AI can easily be reduced or identified
2. **Strict Liability:** Establish a range of new strict liability offenses specifically for AI crimes—i.e., offenses that an AI could commit even in the absence of any mens rea like intent to cause harm, knowledge of an inculpatory fact, reckless disregard of a risk or negligent unawareness of a risk.
	- Can be unfair to punish without someone considering their state of mind. That is, punishing an AI or it's creator through strict liability does not differentiate if they accidentally or deliberately committed the offenses.
3. **A Framework for Direct Mens Rea Analysis for AI:** A framework for directly defining mens rea terms for AI—analogous to those possessed by natural persons—could be crafted.
	- The complexity and variability of AI makes it difficult to establish a consistent standard for what constitutes intent or recklessness within AI systems.
	- Determining the extent to which AI's actions can be considered independent of human influence is a significant challenge.
	- Implementing a direct mens rea analysis for AI would require new types of evidence and methods of interpretation to assess the "mental state" of AI systems. This necessitates expertise in both technology and law, which might not be readily available in all legal contexts.

**Reducibility Challenge**

One might object that there is never a genuine need to punish AI because any time an AI seems criminally culpable in its own right, this culpability can always be reduced to that of nearby human actors—such as developers, owners, and users. The law could target the relevant culpable human actors instead.

- Issues
	- AI actions should be traced back to human misconduct, does not necessarily prevent punishing AI. 
	- It would be impractical and overly invasive for law enforcement to dig into every organization's internal activities for minor faults whenever AI causes harm. 

**Spillover Objection**

It is concerned with the unintended consequences or collateral damage or potential for harm to innocent parties involved with the AI, such as developers, users, or owners, who could be negatively impacted by the AI's punishment.

> What are the feasible alternatives to punishing AI?

**Do nothing (maintain the status quo)**

- In case of AI crimes are reducible, there are legal actions that can be taken on the actor(s). 
	- Such hacker using AI to steal a bank can be shown to have intent and be convicted
	- If a company didn't foresee that AI system could cause harm
		- if the developers or users of AI foresaw a substantial and unjustified risk that an AI will cause the death of a person these human actors could be convicted of reckless homicide.
		- If such a risk was merely reasonably foreseeable (but not foreseen), then lower forms of homicide liability would be available.
	- In case of hard scenarios, where reducible harms by AI that are not foreseeable
		- For example hacker trying to steal the bank using AI inadvertently shuts down the electric grid
		- The cases come under constructive liability crimes where there is base crime which requires mens rea but further crimes where caused without mens rea.
		- If constructive liability was used in the case of hacker bringing the electric grid down, then will be criminally liable for it as well.
	- There is criminal law gap here for AI crimes in this case
- In case AI crimes are irreducible
	- Example: An AI created to buy school supplies for new Harvard students, after analyzing online engineering discussions, unexpectedly learns to order radioactive materials from the dark web, sending them to student housing. The AI's programmers, who had lawful intentions and took reasonable precautions, along with Harvard and its users, did not engage in any criminal activity. Yet, the AI's actions resulted in student fatalities.
		- Both innocent agency and constructive liability are not applicable in this case
	- Even if new laws could be amended these cases, it would hinder innovation and beneficial commercial activities since all technologies involve some risks of harm.
	- So there is gap in the criminal law in these cases

**Cost of Punishing AI**

- Punishment of AI would entail serious practical challenges as well as substantial changes to criminal law.
- Addressing AI culpability may require creating strict liability offenses specifically for AI or developing new legal concepts of AI mens rea. Both approaches demand extensive legislative work and major criminal law revisions.
- Developing a framework to assess AI mens rea involves complex technical and theoretical considerations, including expert testimony on AI behavior and its capacity to weigh legally relevant values.
- Punishing AI necessitates granting them legal personality, introducing a novel form of criminal liability. This significant legal change, mirroring corporate criminal liability, raises issues about the extension of legal rights and obligations to AI.
- **Rights creep:** There's a risk that initially limited legal rights for AI could expand over time, mirroring the extension of rights to corporations, potentially limiting human activities and freedoms.

**Minimally Extending the Criminal Law**

- New Crimes: Introduce laws to criminalize reckless or malicious AI use and failure in AI's responsible design and oversight.
- Responsible Person: Assign a designated individual liable for AI's actions, potentially through new criminal negligence laws.
- Default Rule for Responsibility: Automatically assign responsibility for AI to its manufacturer, supplier, owner, or developer, without mandatory registration.
- Direct Liability: Impose criminal liability on the Responsible Person for failing to adequately oversee the AI.
- Strict Liability: Apply strict liability to the Responsible Person for AI-related crimes, with reservations about its broad application.

**Moderate Changes to Civil Liability**

- Introducing New Liability Rules: Create tailored civil liability rules for AI-related harms.
- Applying Existing Legal Frameworks: Use negligence, product liability, or contractual liability to address AI harms, acknowledging their current limitations.
- Revising Product Liability Directive: The European Commission explores revising this directive to better accommodate new technologies like AI.
- Implementing a Responsible Person Scheme: Assign civil liability to a designated individual responsible for overseeing AI, potentially on a strict liability basis.
- Creating an Insurance Scheme: Develop a fund through taxes on AI use to compensate victims of AI crimes, similar to vaccine injury compensation programs.

# The Boeing 737 MAX

> What are lead to the failure of Boeing 737 MAX

- Boeing faced intense competition from Airbus, prompting a rush to update the 737 instead of designing a new model, affecting design and safety decisions.
- Boeing introduced the MCAS system to address aerodynamic changes due to larger engines but did not fully disclose its functionality or implications to pilots or the FAA.
- The MCAS systems did not go through the necessary FAA's certification process
- The 737 MAX was labelled as update over the existing 737 NG, thereby avoiding the re-certification process

> What were the main design choices that led to the crashes?

1. The analysis underestimated the power of MCAS to move the plane’s horizontal tail and thus how difficult it would be for pilots to maintain control of the aircraft
2. It did not account for the system deploying multiple times
3. It underestimated the risk level if MCAS failed, thus permitting a design feature—the single AOA sensor input to MCAS—that did not have built-in redundancy
4. Not provided sufficient acknowledgement or training for the pilots for the MCAS system

> What is the relationship between Boeing and the FAA? How did this relationship affect what happened leading up to the crashes?

- Boeing introduced the MCAS system to address aerodynamic changes due to larger engines but did not fully disclose its functionality or implications to pilots or the FAA.
- The FAA's certification process relied heavily on Boeing's assessments, with insufficient independent verification of the MCAS system's safety.
- A close relationship between Boeing and the FAA led to a certification process that may have prioritized expedience over rigorous safety evaluation.

> What were the ways discussed in class and in the article to address these issues moving forward?

- Enhance engineers' influence to prioritize safety over profit.
- Focus on moral courage in engineering education and professional development.
	- Support whistleblowers to come forward
- Ensure manufacturers provide full transparency on critical system functionalities.
- Increase regulatory oversight over manufacturers, reducing self-certification.
- Implement clearer accountability and decision-making processes in complex projects.
- Create a formal ethics program and capable ethical leadership.

# Free Speech is a Triangle

> What are few speech controversies mentioned in the article?

- EU "right to be forgotten"
	- Requires search engine companies to eliminate certain newspaper articles from their search results.
- German law - NetzDG
	- It requires social media companies to take down many different kinds of speech, including hate speech, within twenty-four hours of a complaint.
- A third is the concern about fake news propagating through social media sites.
- Charlottesville march
	- Decision by various internet companies to block, censor, or otherwise refuse to do business with various neo-Nazi and hate sites.
 
> Who are the members of the "triangle"?

- **Nation-States and Supranational Organizations**: Governments and entities like the European Union.
- **Internet-Infrastructure Companies**: Includes social media platforms, search engines, ISPs, web-hosting services, DNS registrars, cyber-defense services, and payment systems.
- **Speakers and Legacy Media**: Individual speakers, journalists, media outlets, civil-society organizations, protesters, hackers, and trolls.

> What is EU's "right to be forgotten"?

Requires search engine companies to eliminate certain newspaper articles from their search results. This is to make it hard for people to discover embarrassing stories in newspapers.

> How are the new school regulations compared to the old school regulations?

- Old school regulation -> directed at speakers/association/media company
- New school regulation -> directed at internet infrastructure to surveil, police, and control speech of their users on behalf of the government

> What is collateral censorship?

Collateral censorship occurs when the state targets entity A to control the speech of another entity, B. The state tells A: Locate and block or censor B, or else we will punish or fine you. In effect, collateral censorship attempts to harness a private organization to regulate speech on the state’s behalf.

An example is newspaper being liable for what their reporters write and their advertisers advertise. But this scenario doesn't work for internet companies since the company and it's users are not in same relationship as the newspapers and its reporters.

> What is digital prior restraint?

- Liability imposed on infrastructure providers for not surveilling, blocking, or removing complained-about speech resembles prior restraint, but isn't identical to classic cases.
- Traditional administrative prior restraints prevent speech without a judicial verdict on its protection status or Bill of Rights procedural safeguards.
- In digital contexts, users are silenced until a private company authorizes their speech, if blocked or taken down.
- Digital prior restraint differs as some content is preemptively blocked while others are removed post-publication, and it's enforced by private companies under state pressure, not directly by government officials.

> What is public-private cooperation & co-optation?

Governments attempt to coax, cajole, or coerce private-infrastructure owners to do their bidding and to help them surveil and regulate speech.

> Why is public-private cooperation or co-optation is a consequence of new school speech regulation?

1. 1. Infrastructure owners have superior abilities to identify and remove content, making them preferred by governments for enforcing speech regulations.
2. Modern speech regulation relies on data surveillance to monitor end user activities, necessary for regulating speech.
3. Both nation-states and private infrastructure owners are motivated to support new-school speech regulation. States find it simpler to regulate companies than countless, possibly anonymous, individual speakers.
4. Market dynamics and state pressures have led to the evolution of private governance and surveillance capabilities within infrastructure companies.

As a result, a cycle of governance and cooperation between private companies and governments emerges. Companies enhance their regulatory capabilities, prompting states to leverage these abilities for their regulatory aims, further incentivizing the expansion of surveillance and governance tools by private entities.

> What is private governance?

It refers to the practice of privately owned internet-infrastructure companies, such as social media platforms, governing online speech and behavior through the enforcement of their own policies, rules, and algorithms, effectively taking on roles traditionally associated with state governance

> What are two ways internet companies like Facebook generate growth?

- Significantly grow the user base by offering the service globally.
- Design services to be addictive to capture more of the end users' attention.

For companies like Facebook, growing the membership provides limited growth possibilities, so choose for the second option to make their users to stay and use their app longer.

Simply put, twentieth-century freedom of speech faced a problem of scarcity of access to media. Twenty-first-century freedom of speech faces the problem of scarcity of attention.

> Should private governance be private (according to author)?

According to the author, while private governance is inevitable given the role of digital platforms in public discourse, it should not necessarily remain entirely private or unaccountable. The author suggests that social media companies should recognize and protect free speech and due process values while managing their platforms.

> Why can't First Amendment doctrines can't apply to companies like Facebook?

- **Global Operations**: Facebook's worldwide presence makes U.S. free speech norms inapplicable globally.
- **Governance Needs**: Strict First Amendment application would limit Facebook's ability to manage harmful content effectively.
- **Business Model**: Facebook's model of user engagement and targeted advertising requires content curation, which First Amendment restrictions could impede.
- **Enforcement Limitations**: Facebook's primary enforcement methods (content removal, user bans) could conflict with First Amendment standards if applied directly.

> What is privatized bureaucracy?

It is phenomenon where private digital infrastructure companies, like social media platforms, perform regulatory functions traditionally associated with the state, including content moderation and speech regulation, in response to legal pressures or government demands

> What are the four objections to NetzDG?

- **Substantive Speech Doctrines**: Objection to Germany's hate speech laws as potentially insufficiently protective of free speech.
- **Global Jurisdiction**: Concerns that Germany might enforce its content regulation beyond its geographical boundaries, applying its speech norms globally.
- **Lack of Judicial Determination**: Criticism that under NetzDG, there is no judicial review before private companies block or remove speech, depriving speakers of due process.
- **Surveillance and Manipulation**: Fears that co-opting private infrastructure for speech regulation could also lead to increased digital surveillance and manipulation by the state​.

> What is the best way to protect speech?

The best way to protect free speech values is not to apply doctrines developed for states as rules for private actors. Instead, protecting free speech in a digital age often involves technical, regulatory, and administrative solutions that apply in contexts where the First Amendment does not reach.

> What are the two goals for protecting free speech in a pluralist model?

- Prevent or ameliorate, as much as possible, collateral censorship and new forms of digital prior restraint.
- Protect people from new methods of digital surveillance and manipulation—new methods that emerged from the rise of large multinational companies that depend on the collection, surveillance, analysis, control, and distribution of personal data.

> What are the responsibilities of the different types of private infrastructure?

Different types of private infrastructure have varying responsibilities in the digital age: 
- Social media companies govern online speech and community norms
- Broadband providers manage internet access and data flow 
- Search engines curate information access. 
- Each type plays a crucial role in regulating digital spaces:
	- Social media setting community standards, 
	- Broadband providers ensuring connectivity and network management
	- Search engines influencing information visibility and public discourse.

