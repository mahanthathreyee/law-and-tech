
```table-of-contents
```
# Mozilla v FCC

## Petitioner's Brief

> Are the Petitioners for or against net neutrality?

Petitioners are **FOR** net neutrality.

> What are the main arguments/support for the Petitioners position?

- The FCC wrongly reclassified BIAS from a "telecommunications service" to an "information service," against statutory definitions
- The FCC justified reclassifying BIAS by arguing that since BIAS allows users to access third-party information services, BIAS itself is an information service. 
	- This rationale fails to distinguish between the service's role in providing access to the internet (a telecommunications function) and the nature of the content and services accessed, which are the actual "information services."
- The FCC improperly used functions like DNS and caching to label BIAS as an information service, ignoring their ancillary nature.
- The FCC's repeal of open internet rules neglects its duty to protect public interest and maintain an open internet.
	- This decision undermines the foundational principle of net neutrality, which ensures that BIAS providers do not block, throttle, or unfairly prioritize internet traffic.

## FCC's Brief

> Is FCC for or against net neutrality (in relation to this reading)?

FCC is **AGAINST** net neutrality.

> What are the main arguments/support for the FCC's position?

- The FCC decided to treat broadband as a lighter-regulated "information service," believing this encourages more innovation and investment than the previous, stricter "telecommunications service" classification.
- The FCC ruled that mobile internet is not a "commercial mobile service" needing strict regulation, aiming for consistent treatment with fixed broadband to not over-regulate.
- With broadband reclassified, the FCC removed strict rules, opting for a simpler approach that relies on companies being transparent and existing laws to prevent abuse, which is seen as more efficient.
- The FCC believes that less strict regulation will lead to more broadband investment and doesn't see the need for old rules because competition and legal protections against abuse already exist.
- The FCC decided to override any state or local laws that are stricter than its own, to ensure the internet is governed uniformly across the U.S., preventing a mix of different rules in different places.

# Autonomous Weapons System

> How are autonomous weapons similar and different compared to:
> 1. conventional weapons
> 2. human combatants
> 3. child soldiers
> 4. animal combatants

| **Aspect**             | **Conventional Weapons**                                                                                                          | **Human Combatants**                                                                                                                                         | **Child Soldiers**                                                                                                                | **Animal Combatants**                                                                                                                                 |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Similarity**         | AWS are like conventional weapons in that they can be lawful under specific conditions (e.g., not causing unnecessary suffering). | AWS and human combatants share the capability for independent action on the battlefield.                                                                     | AWS, child soldiers, and animal combatants are capable of autonomous action and may act unpredictably.                            | Similar to child soldiers, AWS and animal combatants can act autonomously and unpredictably.                                                          |
| **Difference**         | AWS's capacity for independent and unpredictable action complicates their legal review and accountability.                        | Unlike humans, AWS cannot be traditionally trained or held accountable for their actions.                                                                    | The legal and societal motivations for regulating child soldiers (to protect children) do not apply to AWS.                       | The rationale for regulating animal combatants does not directly apply to AWS, and there's almost no existing law specifically for animal combatants. |
| **Accountability Gap** | N/A                                                                                                                               | AWS actions that result in violations of international humanitarian law highlight an accountability gap not addressed by the law governing human combatants. | Similar accountability issues as with human combatants; existing frameworks do not accommodate autonomous systems' liability.     | Similar to child soldiers, the unique challenges of AWS accountability are not resolved within the current legal frameworks.                          |
| **Regulation Needs**   | Existing weapons law does not fully address AWS's unique challenges, such as in-field learning and ensuring discriminate use.     | Current combatant law fails to provide guidelines on "training" AWS or addressing the accountability gap for their autonomous actions.                       | Child soldier regulations highlight the need for tailored AWS laws that consider their non-human nature and operational autonomy. | The lack of specific law for animal combatants underscores the necessity for new regulations specifically designed for AWS.                           |

# Trolley and Pinto Problem

> What was the Pinto Problem? 

This problem is exemplified by the controversy surrounding the Ford Pinto in the 1970s, where Ford Motor Company faced criticism for its decision-making process regarding the safety of the vehicle's fuel tank design. Despite being aware of the hazards related to the design, Ford adhered to the original design based on a cost-benefit analysis that compared the costs of making safety improvements against the benefits of preventing fuel tank fires. This analysis calculated the monetary value of human lives and injuries in a way that suggested a willingness to accept human casualties in favor of cost savings.

> How is the Pinto problem similar/different to the Boeing 737 MAX case?

- **Technical and Contextual Nature of Failures:**
    - The Pinto problem was related to a specific design flaw (fuel tank design) that made the car susceptible to catching fire in rear-end collisions.
    - The 737 MAX issue involved a complex flight control system (MCAS) designed to enhance the aircraft's performance, which malfunctioned due to sensor errors and lack of adequate pilot training and information.

- **Regulatory and Industry Environment:**
    - The automotive industry regulations and standards at the time of the Pinto were less comprehensive in addressing safety issues compared to the highly regulated aviation sector during the 737 MAX's development.
    - The Boeing 737 MAX case also involved criticisms of the Federal Aviation Administration (FAA) for its certification process and alleged closeness with Boeing, which was not a significant aspect of the Pinto case.

- **Complexity of Technology and Information Disclosure:**
    - The Boeing 737 MAX's problems were partially attributed to the complexity of modern aviation systems and Boeing's failure to disclose critical information about the MCAS system to pilots and airlines.
    - The Pinto's design issue, by contrast, was a more straightforward safety risk that did not involve the complexities of software or system-wide integration failures.

- **Global Impact and Scale:**
    - The Boeing 737 MAX incidents had a global impact, grounding the worldwide fleet and affecting international air travel, whereas the Pinto issue primarily affected consumers in the United States.
    - The scale of the 737 MAX crisis involved multiple international investigations and a broader scrutiny of global aviation safety practices, while the Pinto controversy was more contained within the automotive industry and national regulatory responses.

# Free Speech in an Algorithmic Society

> Why is Professor Balkin worried about freedom of speech in the current era?

Professor Balkin is concerned about freedom of speech due to the pervasive influence of Big Data and new forms of speech regulation. 

He sees Big Data as a power structure where personal information becomes a commodity, likened to "Soylent Green," leading to surveillance, control, discrimination, and manipulation by both governments and private companies. 

Additionally, the evolution from traditional to new forms of speech regulation, exemplified by the right to be forgotten and the spread of fake news, challenges traditional free speech protections, raising issues around censorship, privacy, and misinformation in the digital age.

> Who does Professor Balkin believe must play a role in preserving freedom of speech?

The best solution would be for large international infrastructure owners and social media platforms to change their self-conception. 

Ideally, they would come to understand themselves as a new kind of media company, with obligations to protect the global public good of a free Internet, and to preserve and extend the emerging global system of freedom of expression. 

Defenders of democratic values should work hard to emphasize the social responsibilities of digital infrastructure companies and help them both to understand and to accept their constitutive role in the emerging global public sphere.

> What are information fiduciaries? What is their role?

Information fiduciaries are entities that have a duty to protect the personal information of individuals they interact with, similar to the fiduciary responsibilities of doctors or lawyers to their clients. 

They must act in the best interests of those whose data they hold, ensuring confidentiality and preventing misuse. This concept suggests that companies collecting and handling significant amounts of personal data should be regulated to enforce ethical standards of trust, confidentiality, and care towards individuals' data.

> Why can't digital information fiduciaries should have fewer obligations than traditional professional fiduciaries according to the author?

1. Their business significantly involves monetizing personal data, unlike traditional fiduciaries.
2. Digital platforms encourage public expression and content creation, affecting how user data is handled and expected to be protected.
3. The level of trust and care expected from digital platforms by users is different; users do not expect digital platforms to protect their interests to the same extent as traditional fiduciaries like doctors or lawyers.

> What is trolling and doxing?

**Trolling:** Refers to the act of deliberately posting provocative or offensive content online to elicit a strong emotional response or disrupt conversations.

**Doxing:** Researching and publicly sharing private or identifying information about an individual without their consent, often to harass, intimidate, or shame them.

> How does Professor Balkin want potentially harmful online phenomena, like trolling and doxing, to be addressed, and by whom?

TBD

> What is an "algorithmic nuisance" and how should this phenomenon be addressed?

**Algorithmic nuisance:** Refers to the negative impact of companies using big data and algorithms to make judgments that affect people's identities, traits, associations, opportunities, and vulnerabilities. It captures the idea of companies externalizing the socially unjustified costs of their decision-making onto individuals. 

To address this phenomenon, it's suggested that there should be regulations aimed at the outputs of algorithmic decision-making, such as discrimination and manipulation, ensuring companies internalize the costs they impose on society and individuals through their practices.

# Big Data's Disparate Impact

> What is disparate treatment?

Unequal treatment of similarly situated people based on a protected class (race, gender, etc.). Disparate treatments comprises of two forms:
- **Formal Discrimination:** Membership in a protected class is used as an input to the model. It covers both the straightforward denial of opportunities based on protected class membership and the use of rational racism.
- **Discriminatory Intent:** Requires evidence that the different treatment was **motivated** by the employee's membership in a protected class, such as race, gender, age, disability, religion, or national origin.

> What is disparate impact?

Unintentional discrimination that occurs when a facially neutral policy or practice disadvantages a protected class (e.g., race, gender, age) to a greater extent than another group.

> What is masking & problems of proof?

**Masking:**

- Masking refers to concealing the intention behind discriminatory decisions in algorithmic decision-making.
- It can involve using data mining to create models that unintentionally or intentionally discriminate, with the process obscuring the discrimination's source.
- Discrimination can be masked by using proxies for protected characteristics, relying on biased historical data, or selecting features that disadvantage specific groups.

**Problems of Proof:**

- Demonstrating discrimination, especially under legal frameworks like disparate impact or treatment, is challenging due to the opacity and complexity of algorithms.
- The indirect ways discrimination manifests in data mining exacerbate these challenges, making it difficult to identify and prove the source of bias or discriminatory intent.
- The article highlights the need for more transparency, accountability, and rigor in algorithmic decision-making to address and prove discrimination effectively.

> Defining the target variable

- Defines what data miners are looking for.
- The process of defining the target variable is crucial in data mining as it sets the goal or outcome that the algorithm seeks to predict or classify.

**Issues:**
- The definition of the target variable is inherently subjective and can introduce biases based on how the problem is framed and understood by those setting up the data mining process.
- Risk of inheriting past biases and discrimination through reliance on historical data.
- The chosen target variable may fail to fully capture the diversity and variations in real-world outcomes.
- Neglect of qualitative aspects in favor of easily measured indicators. leading to outcomes that favor easily measured success indicators over a more holistic understanding of the issue.

> Training data (labeling and data collection)?

**Training Data**
- The dataset used to train the algorithm to make predictions or classifications.
- Consists of examples that the model learns from to understand patterns or regularities.

**Labelling**
- The process of assigning class labels to examples in the training data.
- Crucial for supervised learning, as it guides the algorithm in learning the relationship between input features and the desired output.

**Data Collection**
- Involves gathering the data that will be used for training the model.
- Can include a wide range of sources and methods for acquiring data.

**Issues:**
- Training data may inherit biases from historical decision-making or societal prejudices, leading to biased algorithm outcomes.
- Errors in labeling can misguide the model, leading to inaccurate predictions or classifications.
- Training data that do not accurately represent the diversity of the population or scenario can result in models that perform poorly or unfairly for underrepresented groups.
- Poor data quality, including inaccuracies, incompleteness, or outdated information, can negatively impact model performance and fairness.
- The method of data collection can introduce bias, for example, through the overrepresentation or underrepresentation of certain groups.

> What is feature selection? What are issues related to it?

**Feature Selection**
- The process of identifying and selecting a subset of relevant features (variables, predictors) for use in model construction.
- Aims to improve model performance by including only the most informative, relevant features and excluding redundant or irrelevant data.

**Issues**
- Choosing features that inadvertently exclude or include data can introduce or perpetuate bias in model outcomes
- Features selected may serve as proxies for protected characteristics (e.g., zip code as a proxy for race), leading to indirect discrimination.
- Failure to include features that capture crucial nuances or variations can result in models that do not fully understand or accurately predict outcomes for all groups.
- Selecting features at too coarse a level of detail can mask important distinctions and nuances, impacting model fairness and accuracy.

 > What are proxies? What are issues related to it?
 
 **Proxies:** Proxies in data mining refer to attributes or features used in models that serve as stand-ins for other variables, including protected characteristics like race, gender, or age.

**Issues:**
- **Indirect Discrimination:** Using proxies can lead to indirect discrimination where models make decisions based on attributes closely related to protected characteristics.
- **Unintentional Bias:** Even without explicit intention to discriminate, reliance on proxies can introduce bias into decision-making processes, affecting fairness and equity.
- **Opacity in Decision-Making:** The use of proxies can obscure the true basis for decisions, making it difficult to identify, understand, and address discriminatory practices.
- **Challenge in Identifying Proxies:** Determining what constitutes a proxy can be complex, as many variables could potentially serve as indirect indicators of protected characteristics.

# Punishing Artificial Intelligence

> What are features of AI?

1. AI has the potential to act unpredictably.
2. AI has the potential to act unexplainably. It may be possible to determine what an AI has done, but not how or why it acted as it did.
3. AI may act autonomously. For our purposes, that is to say an AI may cause harm without being directly controlled by an individual.
4. While AI can already outperform people in spectacular fashion in some domains, like board games, in other domains AI is not competitive with toddlers. That is because all AI is designed to perform “narrow” or “specific” tasks.

> Why is reducibility important in AI crimes?

Reducibility is also critical, because if an AI engages in an act that would be criminal for a person and the act is reducible, then there typically will be a person that could be criminally liable.

If an AI act is not effectively reducible, there may be no other party that is aptly punished, in which case intuitively criminal activity could occur without the possibility of punishment.

> What are few scenarios where AI crimes are not reducible?

- **Enforcement Problems:** Identifying the individual responsible for an AI-related crime can be difficult if the perpetrator remains anonymous, such as the creator of a computer virus.
    
- **Practical Irreducibility:** Legal attempts to attribute AI misconduct to specific individuals may be impractical due to the multitude of contributors, the challenge in pinpointing their roles, or their actions being remote in time or location.
    
- **Legal Irreducibility:** Legally, attributing AI crimes to individual actions might not always be advisable. Minor negligent acts by multiple people could cumulatively cause significant harm through AI, but prosecuting these actions might not align with sound criminal policy.

> What is specific and general deterrence?

Specific deterrence is the process whereby punishing a specific individual discourages that person from committing more crime in the future.

General deterrence occurs when punishing an offender discourages other would-be offenders from committing crimes.

> What is the desert constraint?

The desert constraint is the claim that an offender may not, in justice, be punished in excess of his or her desert. 

The main effect of the desert constraint is to rule out punishments that go beyond what is proportionate to one’s culpability. Thus, it would be wrong to execute someone for jaywalking even if doing so would ultimately save lives by reducing illegal and dangerous pedestrian crossings.

> What is an example of an affirmative case for punishing AI?

- Direct punishment of AI could provide unrestricted general deterrence as against the developers, owners, or users of AI and provide incentives for them to avoid creating AIs that cause especially egregious types of harm without excuse or justification.
- Punishment of AI may also have expressive benefits. Expressing condemnation of the harms suffered by the victims of an AI could provide these victims with a sense of satisfaction and vindication.

> What is an issue of punishing AI as a means of expressing condemnation?

Punishing AI could send the message that AI is itself an actor on par with a human being, which is responsible and can be held accountable through the criminal justice system. Such a message is concerning, as it could entrench the view that AI has rights to certain kinds of benefits, protections and dignities that could restrict valuable human activities.

> What is a retributive limitation of this case?

There are 3 limitations:

**Eligibility challenge**

The Eligibility Challenge is simple to state: AI, like other inanimate objects, is not the right kind of thing to be punished. AI lacks mental states and thus cannot fulfill the mental state (mens rea) elements built into most criminal offenses. Therefore, convicting AI of crimes requiring a mens rea like intent, knowledge or recklessness would violate the principle of legality.

There are 3 ways to address this challenge:
1. **Respondent Superior:** Suggests that culpable mental states of human actors (developers, owners) associated with an AI could be imputed to the AI under certain conditions, making the AI liable for crimes.
	- Doesn't work for hard AI crimes where actors behind the AI can easily be reduced or identified
2. **Strict Liability:** Establish a range of new strict liability offenses specifically for AI crimes—i.e., offenses that an AI could commit even in the absence of any mens rea like intent to cause harm, knowledge of an inculpatory fact, reckless disregard of a risk or negligent unawareness of a risk.
	- Can be unfair to punish without someone considering their state of mind. That is, punishing an AI or it's creator through strict liability does not differentiate if they accidentally or deliberately committed the offenses.
3. **A Framework for Direct Mens Rea Analysis for AI:** A framework for directly defining mens rea terms for AI—analogous to those possessed by natural persons—could be crafted.
	- The complexity and variability of AI makes it difficult to establish a consistent standard for what constitutes intent or recklessness within AI systems.
	- Determining the extent to which AI's actions can be considered independent of human influence is a significant challenge.
	- 1Implementing a direct mens rea analysis for AI would require new types of evidence and methods of interpretation to assess the "mental state" of AI systems. This necessitates expertise in both technology and law, which might not be readily available in all legal contexts.

**Reducibility Challenge**

One might object that there is never a genuine need to punish AI because any time an AI seems criminally culpable in its own right, this culpability can always be reduced to that of nearby human actors—such as developers, owners, and users. The law could target the relevant culpable human actors instead.

- Issues
	- AI actions should be traced back to human misconduct, does not necessarily prevent punishing AI. 
	- It would be impractical and overly invasive for law enforcement to dig into every organization's internal activities for minor faults whenever AI causes harm. 

**Spillover Objection**

It is concerned with the unintended consequences or collateral damage or potential for harm to innocent parties involved with the AI, such as developers, users, or owners, who could be negatively impacted by the AI's punishment.

> What are the feasible alternatives to punishing AI?

**Do nothing (maintain the status quo)**

- In case of AI crimes are reducible, there are legal actions that can be taken on the actor(s). 
	- Such hacker using AI to steal a bank can be shown to have intent and be convicted
	- If a company didn't foresee that AI system could cause harm
		- if the developers or users of AI foresaw a substantial and unjustified risk that an AI will cause the death of a person these human actors could be convicted of reckless homicide.
		- If such a risk was merely reasonably foreseeable (but not foreseen), then lower forms of homicide liability would be available.
	- In case of hard scenarios, where reducible harms by AI that are not foreseeable
		- For example hacker trying to steal the bank using AI inadvertently shuts down the electric grid
		- The cases come under constructive liability crimes where there is base crime which requires mens rea but further crimes where caused without mens rea.
		- If constructive liability was used in the case of hacker bringing the electric grid down, then will be criminally liable for it as well.
	- There is criminal law gap here for AI crimes in this case
- In case AI crimes are irreducible
	- Example: An AI created to buy school supplies for new Harvard students, after analyzing online engineering discussions, unexpectedly learns to order radioactive materials from the dark web, sending them to student housing. The AI's programmers, who had lawful intentions and took reasonable precautions, along with Harvard and its users, did not engage in any criminal activity. Yet, the AI's actions resulted in student fatalities.
		- Both innocent agency and constructive liability are not applicable in this case
	- Even if new laws could be amended these cases, it would hinder innovation and beneficial commercial activities since all technologies involve some risks of harm.
	- So there is gap in the criminal law in these cases

**Cost of Punishing AI**

- Punishment of AI would entail serious practical challenges as well as substantial changes to criminal law.
- Addressing AI culpability may require creating strict liability offenses specifically for AI or developing new legal concepts of AI mens rea. Both approaches demand extensive legislative work and major criminal law revisions.
- Developing a framework to assess AI mens rea involves complex technical and theoretical considerations, including expert testimony on AI behavior and its capacity to weigh legally relevant values.
- Punishing AI necessitates granting them legal personality, introducing a novel form of criminal liability. This significant legal change, mirroring corporate criminal liability, raises issues about the extension of legal rights and obligations to AI.
- **Rights creep:** There's a risk that initially limited legal rights for AI could expand over time, mirroring the extension of rights to corporations, potentially limiting human activities and freedoms.

**Minimally Extending the Criminal Law**

- New Crimes: Introduce laws to criminalize reckless or malicious AI use and failure in AI's responsible design and oversight.
- Responsible Person: Assign a designated individual liable for AI's actions, potentially through new criminal negligence laws.
- Default Rule for Responsibility: Automatically assign responsibility for AI to its manufacturer, supplier, owner, or developer, without mandatory registration.
- Direct Liability: Impose criminal liability on the Responsible Person for failing to adequately oversee the AI.
- Strict Liability: Apply strict liability to the Responsible Person for AI-related crimes, with reservations about its broad application.

**Moderate Changes to Civil Liability**

- Introducing New Liability Rules: Create tailored civil liability rules for AI-related harms.
- Applying Existing Legal Frameworks: Use negligence, product liability, or contractual liability to address AI harms, acknowledging their current limitations.
- Revising Product Liability Directive: The European Commission explores revising this directive to better accommodate new technologies like AI.
- Implementing a Responsible Person Scheme: Assign civil liability to a designated individual responsible for overseeing AI, potentially on a strict liability basis.
- Creating an Insurance Scheme: Develop a fund through taxes on AI use to compensate victims of AI crimes, similar to vaccine injury compensation programs.







